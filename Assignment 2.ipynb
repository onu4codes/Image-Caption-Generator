{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of missing data in pos_data 0\n",
      "no. of missing data in neg_data 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import csv\n",
    "import re\n",
    "data=[]\n",
    "directory=r'C:\\Users\\mvvha\\OneDrive\\Desktop\\soc\\aclImdb_v1\\aclImdb\\train\\pos'\n",
    "for i in os.listdir(directory):\n",
    "    if i.endswith(\".txt\"):\n",
    "         file_path = os.path.join(directory,i)\n",
    "         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "              content=file.read()\n",
    "              data.append(content)\n",
    "rev=[\"positive\"]*len(data)\n",
    "pos_data={\"Review\":data,\"sentiment\":rev}\n",
    "positive_data=pd.DataFrame(pos_data)\n",
    "data=[]\n",
    "directory=r'C:\\Users\\mvvha\\OneDrive\\Desktop\\soc\\aclImdb_v1\\aclImdb\\train\\neg'\n",
    "for i in os.listdir(directory):\n",
    "    if i.endswith(\".txt\"):\n",
    "         file_path = os.path.join(directory,i)\n",
    "         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "              content=file.read()\n",
    "              data.append(content)\n",
    "rev=[\"negative\"]*len(data)\n",
    "neg_data={\"Review\":data,\"sentiment\":rev}\n",
    "negative_data=pd.DataFrame(neg_data)\n",
    "csv_file_path_1 = './positive.csv'\n",
    "csv_file_path_2= './negative.csv'\n",
    "positive_data.to_csv(csv_file_path_1, index=False)\n",
    "negative_data.to_csv(csv_file_path_2,index=False)\n",
    "print(\"no. of missing data in pos_data\",positive_data['Review'].isnull().sum())\n",
    "print(\"no. of missing data in neg_data\",negative_data['Review'].isnull().sum())\n",
    "positive_data['Review']=positive_data['Review'].str.lower()\n",
    "negative_data['Review']=negative_data['Review'].str.lower()\n",
    "def remove_html_tags(text):\n",
    "     pattern=re.compile('<,*>')\n",
    "     return pattern.sub(r' ',text)\n",
    "positive_data['Review']=positive_data['Review'].apply(remove_html_tags)\n",
    "negative_data['Review']=negative_data['Review'].apply(remove_html_tags)\n",
    "import string \n",
    "punch=string.punctuation\n",
    "def remove_punc(text):\n",
    "     return text.translate(str.maketrans(\" \",\" \",punch))\n",
    "positive_data['Review']=positive_data['Review'].apply(remove_punc)\n",
    "negative_data['Review']=negative_data['Review'].apply(remove_punc)\n",
    "def remove_extra_white(text):\n",
    "     cleaned_line = re.sub(r'\\s+', ' ', text).strip()\n",
    "     return cleaned_line\n",
    "positive_data['Review']=positive_data['Review'].apply(remove_extra_white)\n",
    "negative_data['Review']=negative_data['Review'].apply(remove_extra_white)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7219    [okay, like, many, film, spawn, snl, skit, tim...\n",
      "4232    [film, leave, feel, gregor, jordans, ned, kell...\n",
      "Name: Review, dtype: object\n",
      "6610    finish watch movie maybe 7th 8th time pick one...\n",
      "9489    typically watch short film always afraid perso...\n",
      "Name: cleaned_review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def tokenize_data(text):\n",
    "    tokens=text.split(\" \")\n",
    "    return tokens\n",
    "positive_data['Review']=positive_data['Review'].apply(tokenize_data)\n",
    "negative_data['Review']=negative_data['Review'].apply(tokenize_data)\n",
    "def remove_stop_words(text):\n",
    "    stop_words=stopwords.words(\"english\")\n",
    "    text_out=[]\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            text_out.append(word)\n",
    "    return text_out\n",
    "positive_data['Review']=positive_data['Review'].apply(remove_stop_words)\n",
    "negative_data['Review']=negative_data['Review'].apply(remove_stop_words)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "def lemmatize_data(text):\n",
    "    z=[]\n",
    "    for word in text:\n",
    "        out=lemmatizer.lemmatize(word,pos=\"v\")\n",
    "        z.append(out)\n",
    "    return z\n",
    "# text = [\"running\", \"ate\", \"better\", \"cats\"]\n",
    "# lemmatized_text = lemmatize_data(text)\n",
    "# print(lemmatized_text)\n",
    "positive_data['Review']=positive_data['Review'].apply(lemmatize_data)\n",
    "negative_data['Review']=negative_data['Review'].apply(lemmatize_data)\n",
    "print(positive_data['Review'].sample(n=2))\n",
    "positive_data[\"cleaned_review\"]=positive_data['Review'].apply(lambda x:\" \".join(x))\n",
    "negative_data['cleaned_review']=negative_data['Review'].apply(lambda x:\" \".join(x))\n",
    "print(positive_data['cleaned_review'].sample(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13271    ok read film somewhere internet many criticize...\n",
      "20704    could say better gymkata least felt money tota...\n",
      "Name: cleaned_review, dtype: object\n",
      "Document-Term Matrix (TF-IDF):\n",
      "\n",
      "Vocabulary Mapping (Word to Index):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "df_pos = pd.DataFrame(positive_data['cleaned_review'])\n",
    "df_neg=pd.DataFrame(negative_data['cleaned_review'])\n",
    "positive_data['label']=[1]*len(positive_data)\n",
    "negative_data['label']=[0]*len(negative_data)\n",
    "net_df = pd.concat([positive_data,negative_data], ignore_index=True)\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train= vectorizer.fit_transform(net_df['cleaned_review'])\n",
    "print(net_df['cleaned_review'].sample(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes Classifier: 0.84\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84     12500\n",
      "           1       0.84      0.82      0.83     12500\n",
      "\n",
      "    accuracy                           0.84     25000\n",
      "   macro avg       0.84      0.84      0.84     25000\n",
      "weighted avg       0.84      0.84      0.84     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos_directory = r'C:\\Users\\mvvha\\OneDrive\\Desktop\\soc\\aclImdb_v1\\aclImdb\\test\\pos'\n",
    "neg_directory = r'C:\\Users\\mvvha\\OneDrive\\Desktop\\soc\\aclImdb_v1\\aclImdb\\test\\neg'\n",
    "def load_data_from_directory(directory, sentiment):\n",
    "    data = []\n",
    "    for i in os.listdir(directory):\n",
    "        if i.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory, i)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                data.append(content)\n",
    "    labels = [1 if sentiment == \"positive\" else 0] * len(data)\n",
    "    return pd.DataFrame({\"Review\": data, \"sentiment\": sentiment, \"label\": labels})\n",
    "\n",
    "# Load positive and negative test data\n",
    "positive_data_test = load_data_from_directory(pos_directory, \"positive\")\n",
    "negative_data_test = load_data_from_directory(neg_directory, \"negative\")\n",
    "# Combine positive and negative test data\n",
    "net_test = pd.concat([positive_data_test, negative_data_test], ignore_index=True)\n",
    "net_test[\"Review\"]=net_test[\"Review\"].apply(remove_html_tags)\n",
    "net_test[\"Review\"]=net_test[\"Review\"].apply(remove_punc)\n",
    "net_test[\"Review\"]=net_test[\"Review\"].apply(remove_extra_white)\n",
    "net_test[\"Review\"]=net_test[\"Review\"].apply(tokenize_data)\n",
    "net_test[\"Review\"]=net_test[\"Review\"].apply(remove_stop_words)\n",
    "net_test[\"Review\"]=net_test[\"Review\"].apply(lemmatize_data)\n",
    "net_test[\"clean_review\"]=net_test[\"Review\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# Transform the test data using the fitted vectorizer\n",
    "X_test = vectorizer.transform(net_test['clean_review'])\n",
    "\n",
    "# Get the labels\n",
    "y_train = net_df['label']\n",
    "y_test = net_test['label']\n",
    "\n",
    "# Initialize Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "nb_classifier.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Naive Bayes Classifier: {accuracy:.2f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
